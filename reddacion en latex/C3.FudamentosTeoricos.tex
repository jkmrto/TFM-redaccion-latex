%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Documento LaTeX 																						%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Título:		Capítulo 2
% Autor:  	Ignacio Moreno Doblas
% Fecha:  	2014-02-01
% Versión:	0.5.0
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapterbegin{Fundamentos Teóricos}
\label{chp:Utiliz}
%\minitoc

\par El trabajo realizado en este proyecto es englobado dentro de la temática denomiada visión por computador, dado que los métodos empleados se basan en la detección de patrónes sobre las imágenes dadas, en nuestro caso neuroimágenes. 

\par En el ámbito de la visión por computador cada imagen en sí misma es una muestra de miles dimensiones, cada uno de los pixeles. Un modelo generativo trata de capturar la relación entre las multiples dimensiones de los datos. En nuestro caso el modelo empleado para capturar dichas relaciones es el Autoencoder Variacional.

\par Es por ello que este capítulo se centrará en la exposición de este método en primer lugar. Dado el VAE esta fundamentado en el aprendizaje profundo, se dedicará la siguiente sección a las redes neuronales, haciendo especial hincapie a aquellas empleadas en este trabajo. Finalmente se expondrán brevemente los métodos estadísticos usados de manera auxiliar a lo largo de este proyecto.  

\section{Autoencoder Variacional}

\par Este apartado está dedicado a la exposición del  Autoencoder Variacional desde una perspectiva meramente teórica  con objeto de mostrar los fundamentos y, en última instancia, la capacidad de convergencia del método, basada en una función objetivo sobre la cual se puede aplicar descenso en gradiente estocástico. 

\subsection{Modelo de Variables Latentes}

\par

\begin{figure}[htp]
\centering
\includegraphics[scale=0.25]{images/ModeloVariablesLatentes.png}
\caption{Modelo gráfico de variables latentes para el modelo generativo del VAE. $Z$ es el espacio de variables lo más similar posible a un distribución normal $(N(0,I))$. El elemento $\theta$ es el conjunto de parámetros que aplicados de manera funcional sobre las variables latentes son capaces de generar el conjunto muestral $X$ }
\label{latentes_variables}
\end{figure}

\par A lo largo del entrenamiento o la caracterización de un modelo generativo, la parte mas complicada es la extracción de las dependencias entre las múltiples dimensiones. Son estas relaciones multidimensionales las que permiten generar muestras artificiales pertenecientes a clases distintas. Se denomina variable latente, a las unidades del modelo generativo capaces de discernir entre las distintas clases, esto es, capacitan al modelo para generar elementos diferenciados.



\par Un modelo generativo es representativo de un espacio muestral $(X)$ si para cada una de las muestras de dicho espacio $(x)$ hay al menos alguna configuración de las variables latentes $(z)$ que genera un variable $(\hat{x})$ muy similar a la original. Formalmente, dada una función  $f(z, \theta)$ parametrizada por un vector $\theta$ en un espacio $\Theta$ tal que:
\begin{center}
\begin{equation} \label{eq:space}
f : 	Z \times \Theta  \rightarrow  X  
\end{equation}
\end{center}

\subsection{Modelo Probabilístico}
\par El objetivo es maximizar la probabilidad de cada $x$ de el espacio muestral de acuerdo con:
\begin{center}
\begin{equation} \label{eq:int_1}
P(X)  = \int_{}^{}P(X|z;\theta) 
\end{equation}
\end{center}
\par En la ecuación \ref{eq:int_1}, $f(z;\theta)$ es reemplazada por la distribución $P(X| z;\theta)$, la cual nos permite hacer explícita la dependencia de $X$ sobre $z$, debido a la probabilidad condicionada. 
La idea de detrás de dicha expresión es principio de máxima verosimilitud (ML, del inglés \textit{Maximum Likehood}), el cual indica que si el modelo es capaz de generar muestras del espacio $X$, entonces será probable que le modelo generativo construya muestras similares.

\par En el VAE, la función de probabilidad $P(X|z;\theta)$ es las siguiente:

\begin{center}
\begin{equation} \label{eq:p_x_gausiana}
P(X|z; \theta)  = N(X| f(z;\theta), \sigma^{2}*I) 
\end{equation}
\end{center}

\par El uso de una distribución gausiana nos permite emplear descenso en gradiente durante la optimización, con objeto de caracterizar el modelo. Esta caracterización permite incrementar $P(X)$, entendidada como la probabilidad global de generar algún tipo de muestra de dicho espacio. Esto no sería posible si esta función de probabilidad fuera una delta de Dirac. Es importante notar que es fundamental disponener de una función $P(X|z)$ que sea computable y continua sobre $\theta$.

\par Teóricamente, para la mayoría de los valores $z$, $P(X|z)$ será aproximadaente cero, y por lo tanto su contribuciónpara la estimación de $P(X)$ será prácticamente nula.

\subsection{Función Objetivo}

\par La principal idea en la que se fundamenta el VAE es en muestrear los valores de $z$ a partir de $X$, esto es, necesitamos una nueva función $Q(z|X)$ que nos permita generar el conjunto de valores del espacio $Z$ a paritr de $X$. Esto nos reduce el espacio de $Z$ ya que, teóricamente, este se verá limitado en $Q(z|X)$. En última instacia, esto nos permitirá estimar  $E[P(X|z)]$, siendo esta el valor esperado de la distribución de probabilidad de los valores de X generados. 
\par La relacion entre $E(P(X|z))$ y $P(X)$ es uno de los fundamentos de los métodos variacionales Bayesianos. Comencemos con la definición de la divergencia de KUllback-Leibler (\textit{KL}  o \textit{D}) entre una distribución $P(z|X)$ y $Q(z)$:


\begin{center}
\begin{equation} \label{eq:KL_1}
D[Q(z)||P(z|X)] = E[log(Q(x)) - log(P(z|X))] 
\end{equation}
\end{center}

\par La expresión anterior, ecuación  \ref{eq:KL_1}, es una medida no simétrica de la similitud o diferencia entre las dos funciones de probabilidad $P(X|z) y Q(z)$. Dicha expresión mide diferencia (o el extra de información) entre un coódigo $P(x)$ y uno $Q(z)$. Aplicando la regla de Bayes sobre la expresión anterior conseguimos dejarlo en función de $P(X)$ y $P(X|z)$:

\begin{center}
\begin{equation} \label{eq:KL_2}
D[Q(z)||P(z|X)] = E_{z}[log(Q(x)) - log(P(X|z)) - log(P(z)) + log(p(X))]  
\end{equation}
\end{center}

\par Ordenando la expresión anterior, y teniendo en cuenta que $log(p(X))$ no depende de $z$ por lo que puede salir del valor esperado:

\begin{center}
\begin{equation} \label{eq:KL_3}
log(p(X))- D[Q(z)||P(z|X)]  = E_{z}[log(P(X|z)]) - D[Q(z)||P(z)].  
\end{equation}
\end{center}

\par Llegados a este punto es importante notar que el espacio $X$ es fijo y por lo tanto también lo  es su función de probabilidad $P(X)$. No obstante $Q(z)$ puede ser cualquier distribución, siempre que nos permita generar $Z$ a partir de $X$.
\par Dado que en nuestro caso estamos intersados en inferir $P(X)$, es necesario generar una función $Q$ dependiente sobre $X$ que permita que la divergencia $D[Q(z)||P(z|X)]$ sea pequeña, esto es, haya la menor perdida de información entre ambas distribuciones.

 \begin{center}
\begin{equation} \label{eq:KL_4}
log(p(X))- D[Q(z|X)||P(z|X)]  = E_{z}[log(P(X|z)]) - D[Q(z|X)||P(z)].  
\end{equation}
\end{center}

\par La expresión anterior, ecuación \ref{eq:KL_4}, es la principal del VAE, por lo que es necesario examinarla detenidamente. Analizando cada término por separado:
\begin{itemize}
\item La expresión de la izquierda representa la cantidad que se prentende maximizar: $log(P(x))$, mas un término de error reperesentado por D[Q(z)||P(z|X)] que es la capacidad de generar $z$ a partir de $X$. Este término de error será disminuido si $Q$ es de alta capacidad. 
\par Se trata de maximizar $log(P(X))$ mientras simultaneamente $D[Q(z|X)||P(z)]$ se minimiza . El término de probabilidad $P(z|X)$ no es computable analíticamente, describe la distribución de valores de $z$ que son capacades de generar $X$.
\item La expresión de la derecha es lo que se pretende optimar mediante el descenso en gradiente, dada una correcta seleccion de $Q(x)$.
\par Este segundo término fuerza la similitud entre $(Q(z|X))$ y$ P(X|z)$. Asumiendo que el término $Q(z|X)$ es de alta capacidad, tendrémos que el término de divergencia KL será cercano a cero. En última instancia, conseguiremos manejar de forma ana?itica $P(z|X)$ gracias a su similitud con $Q(z|X)$ 
\end{itemize}

\subsection{Optimización de la función objetivo}

\par Con objeto de poder realizar el descenso en gradiente sobre la expresión de la derecha de la ecuación \ref{eq:KL_4}, necesitamos definir de manera más exacta la forma de $Q(z|X)$. La elección habiutual es la siguiente:

 \begin{center}
\begin{equation} \label{eq:OFU_1}
Q(z|X)  = N(z|\mu(X;\vartheta), \Sigma(X; \vartheta))  
\end{equation}
\end{center}

donde $\mu$ y $\Sigma$ son funciones determistas con una serie de parámetros $\vartheta$ (en las siguientes expresiones se omitirá $\vartheta$). Normalmente tanto $\mu$ como $\Sigma$ son implementados mediante redes neuronales y $\Sigma$ esta limitada a un función diagonal, que permite facilitar los cálculos.

\par El segundo término de la expersion \ref{eq:KL_4}, $D[Q(z|X)||P(z)]$, al ser una divergencia KL entre dos funciones de gausianas multivaradas queda definda por:


 \begin{align*}
 \begin{split}
D(N(\mu_{0}(X), \Sigma_{0}(X)|| N(\mu_{1}(X), \Sigma_{1}(X)))  = \\
\frac{1}{2}\left(tr(\Sigma_{1}^{-1}\Sigma_{0}) + (\mu_{1} - \mu_{0})^{T} \Sigma_{1}^{-1}(\mu_{1} - \mu_{0}) - k + log (\frac{det\Sigma_{1}}{det\Sigma_{0}})  \right)
\end{split} 
 \end{align*}  \label{eq:OFU_2}
 
donde $k$ es la dimensionalidad de la distribución, la expresión queda de la siguiente manera:

 \begin{align}
 \begin{split}
D[N(\mu(X), \Sigma(X)) || N(0, I))] = \\
\frac{1}{2} \left( tr(\Sigma(X)) + (\mu(X))^{T}(\mu(X) - k -log(det(\Sigma(X)))) \right)
\end{split} 
 \end{align}  \label{eq:OFU_3}

\par El primer término de la expresion \ref{eq:KL_4}, $E_{z}[log(P(X|z))]$, es algo más complicado de determinar, aunque a priori se podría estimar usando un número suficentes de $z$ y aplicando al función $f$ asociada a $P(X|z)$, aunque esto sería tremendamente costoso computacionalmente. 
\par En  su lugar, se aplica un procedimiento denomidado Descenso en Gradiente Estocástico (SGD, del inglés \textit{Stochastic Gradient Descent}), que se basa en tomar úmicamente un valor de $z$ aplicarlo sobre $P(X|z)$, por lo que se obtendría una aproximación de $E_{z}[log(P(X|z))]$. Durante este proceso, estamos tomando como referencia cada una de las muestras $X$ de un conjunto de datos $D$ a la hora de estimar el error. Teniendo en cuento esto, la ecuación completa que se pretnede optimizar es:

 \begin{align}
 \begin{split}
E_{X}[log(P(X)) - D[Q(z|X) || P(z| X)]] = \\
E_{X}[E_{z}[log(P(X|z))] - D[Q(z|X) || P(z)]]
\end{split} 
 \end{align}  \label{eq:OFU_4}

\par Tomando el gradiente de la expresión anterior, reducimos la expresión a los valores internos de las esperanzas. Además, podemos tomar un único valor de $X$ y un único valor de $z$ de la distribución $Q(z|X)$, lo que no nos permite hacer computable el gradiente de la siguiente forma: 

 \begin{center}
\begin{equation} \label{eq:OFU_5}
log(P(X|z)) - D[Q(z|X) || P(z)]. 
\end{equation}
\end{center}

\par No obstante hay un problema significativo en la ecuación \ref{eq:OFU_4} ya que $E_{z}[log(P(X|z))]$
depende de los parámetros de $P$ y también de los valores de $Q$. Esto es problemático a la hora de realizar el descenso en gradiente, quedando resuelto con lo que se conoce como "Truco de Reparametrización".

\subsection{El truco de Reparametrización} \label{sec:repa_1}

\begin{figure}[htp]
\centering
\includegraphics[scale=0.4]{images/trucoReparametrizacion.png}
\caption{(Izquierda) Modelo de VAE sin Truco de Reparametrización. (Derecha) Modelo de VAE con Truco de Reparametrización}
\label{fig:repara_trick}
\end{figure}

\par Para garantizar el correcto funcionamiento del VAE es necesario que la función codificadora ($f$) asociada a $Q(z|X)$ generare un conjunto $Z$ capaz de ser decodificado por la función generadora ($g$) asociada a $P(X|z)$.
\par Analizando el problema desde otra perspectiva, tomando como referencia el diagrama de izquierda de la figura \ref{fig:repara_trick}. El paso hacia delante\footnote{En el ámbito de las redes neuronales se denomina paso hacia delante (del ingés \textit{forward pass}) al proceso incial de evaluar la salida generada a partir de una determinada entrada. En nuestro caso la entrada es $X$ y la salida $f(z)$, siendo la evaluación realizada $||X - f(z)||$} funciona de manera de correcta y es de esperar (si los parámetros están correctamente entrenados) que la salida produzca un salida acertada de manera general. 
\par No obstante, es necesario realizar el paso hacia atrás\footnote{En el ámbito de las redes neuronales, el paso hacia atrás (del inglés \textit{backpropagation}) hace referencia al proceso de evaluar el gradiente en cada uno de los elementos del sistema, tomando como referencia que el error se?a el determinado del paso hacia delante} teniendo que determinar el gradiente sobre la función $Q(z|X)$ encargada de generar $z$, pero este modelo de generación esta basado en el mapeo sobre una distribución gausiana, lo cual es una función no continua.

\par La solución a este problema se denomica truco de reparametrización (del inglés \textit{reparameterization trick}) el cual se basa en trasladar el mapeo sobre la distribución gausiana a una capa de entrada. 
\par Dados $\mu_{X}$ y  $\Sigma_{X}$, media y convarianza respectivamente de $Q(z|X)$, podemos mapear $N(\mu_{X}, \sigma(X))$ tomando un valor de la función Normal ($\epsilon \sim N(0,I)$) y aplicando la siguiente expresión:
\begin{center}
\begin{equation} \label{eq:RT_1}
z = \mu(X) + \Sigma(X)^{1/2} * \epsilon. 
\end{equation}
\end{center}

\par Por lo tanto la función final, la cual queda representada en el diagrama de la derecha de la figura  \ref{fig:repara_trick}, sobre la que se aplica el gradiente es la siguiente:

 \begin{align}
 \begin{split}
E_{X\sim Z}\left[E_{\epsilon\sim N(0,I)}[log(P(X|z = \mu(X) + \Sigma^{1/2} * \epsilon))] - D[Q(z|X) || P(z)]\right].
\end{split} 
 \end{align}  \label{eq:RT_2}

\par Cabe notar que ninguna de las esperanzas son con respecto a las distribuciones características del sistema (ni $P(X|z)$ ni $Q(z|X)$) lo que nos permite realizar el gradiente sin ningún problema sobre los elementos contenidos dentro de los valores esperados, ya que el gradiente es la derivada sobre los parámetros funcionales de estas distribuciones.
\par Por lo tanto dado un valor de $X$ y $\epsilon$ la función \ref{eq:RT_2} será continua y determinista sobre los parámetros de $P$ y $Q$, lo cual nos permite realzar el paso hacia atrás de manera eficaz. 
 
 
\subsection{Interpretación de la función objetivo} 


\begin{figure}[htp]
\centering
\includegraphics[scale=0.25]{images/FO_interpretacion_2.png}
\caption{Traducir }
\end{figure}


\begin{figure}[htp]
\centering
\includegraphics[scale=0.25]{images/FO_interpretacion_1.png}
\caption{Traducir }
\end{figure}


 
\subsection{Codificación y Decodificación}

\par La eficacia y tratabildad del método reside en la asunción de que $Q(z|X)$, la función coficadora, puede ser modelada como una gaussiana con una media determinada $\mu(X)$  y varianza $\Sigma(X)$, por otro lado es necesario que $P(X)$ converja de manera eficaz a la distribución real de los datos del espacio $D$. Estas condiciones solo son superadas si y solo si $D[Q(z|X) || P(z|X)]$ es cercana a cero.
\par Es por ello necesario una función $Q$ de alta capacidad, lo cual puede llevarnos a modelos complejos. Los modelos basados en funciones usados en los VAE son las redes neuronales


\begin{figure}[htp]
\centering
\includegraphics[scale=0.7]{images/decodificador,codificador.png}
\caption{Esquematización simple de las funciones del Codificador y el Decodificador en el VAE}
\label{fig:CD_1}
\end{figure}

\par El codificador es una red neuronal. Su entrada es el dato $X$ y su salida es la representación latente $z$. Representa la distribución de probabilidad $Q(z|X)$, y esta determinada por el conjunto de parámetros y pesos de la red neuronal asociada. Denominaremos a la función encargada de la codificación $q_{\theta}(z|x)$
\par El codificador se identifica a menudo con el proceso de reducción de la dimensionalidad de $x$ a $z$. Cabe notar que el codificador tiene asociadas dos funciones, una encargada de obtener la media $q_{\mu}(X)$  y otro la varianza $q_{\Sigma}(X) $ del espacio latente. Para la obtención final de $z$ se ha de aplicar  el truco de reparametrización, ver seccion \ref{sec:repa_1}, con respecto a los valores $\Sigma$ y $\mu$ obtenidos anteriormente.

\par El decodificador es otra red neuronal. Su entrada es la variable del espacio latente $z$ y su salida es la reconstrucción del dato inicial $X$. Denominaremos a la función encargada de la decodificación $p_{\phi}(x|z)$, donde $\phi$ son el conjunto de parámetros y pesos que definen la red neuronal.

\par El hecho de que ambas funciones estén basadas en redes neuronales hace el aprendizaje profundo sea una parte primordial del VAE. Típicamente los formatos de redes neuronales aplicados en este sistema son dos; redes neuronales densas (DNN)  o redes neuronales convolucionales (CNN).

\newpage
\section{Redes Neuronales}
\par Las Redes Neuronales permiten generar funciones complejas no lineales gracias a su capacidad inherente de aprendizaje con el proceso denomiado propagación hacia atrás, que permiten ajustar los pesos de las distintas unidades o neuronas del sistema. 
\par Dada la complejidad del ámbito del aprendizaje profundo, en las siguiente seciones se pretenden exponer las ideas fundamentales para comprender el comportamiento de las funciones de codificacion y decodificación del VAE, sin entrar en explicaciones excesivamente teóricas sobre los fundamentos de las redes neuronales. 
\par Es por ello que en primer lugar se expondrá el modelo de redes neuronales densas, aprovechando para exponer de manera somera algunos conceptos de redes neuronales, como son el concepto de funciones de activación o el proceso de propagación hacia atrás. 

\par Seguidamente se expondrá el otro modelo de aprendizaje profundo utilizado en este trabajo que son las redes neuronales convolucionales, explcando por que son ideales para la captura de patrones sobre imágenes. 

\subsection{Red Neuronal Densa}
\par Este modelo constituye el paradigma básico de redes neuronales. Fundamentado en el estándar de neuronal artificial  según los principios descritos Rumelhart y McClelland en 1986 \cite{DNN_1}. Siguiendo dichos principios, la i-ésima neurona artificial  consiste en:

\begin{figure}[!hb]
\centering
\includegraphics[scale=0.5]{images/DNN_1.png}
\caption{Sistema global de proceso de una red neuronal}
\label{fig:CD_1}
\end{figure}


\begin{itemize}

\item Un conjunto de entradas $x_j$ con un conjunto de pesos sinápticos asociados $w_{ij}$, con $j=1,2...n$
\item Una regla de propagación $h_i$ a definida partir del conjunto de entradas  y de los pesos sinápticos. Normalmente la regla de propagación utilizada el producto lineal entre los pesos sinápticas y las entradas. Esto es:
\begin{center}
\begin{equation}\label{eq:DNN_1}
h_i(x_1,.....,x_m, w_{i1}....,w{in} = \sum_{i=1}^{n}w_{ij}*x_{j}
\end{equation}
\end{center}

\item Una función de activación, la cual representa simultáneamente la salida de la neurona y su estado de activación. Denotando por $y_i$ dicha función de activación:

\begin{center}
\begin{equation} \label{eq:DNN:2}
y_i = f_i(h_i) = f_i(\sum_{j=0}^{n}w_{ij}x_{j})
\end{equation}
\end{center}  

\end{itemize} 

\subsubsection{Función de Activación}
\begin{figure}[!b]
\centering
\includegraphics[scale=0.45]{images/functions_activations.png}
\caption{Principales funciones de activación.}
\label{fig:CD_1}
\end{figure}
\par La eleccion de la función de activación constituye una parte determinante en el diseño de redes neuronales, dado que afectará en gran medida al a capacidad de decisiónd de la red y la rapidez con que la red sea capaz de converger durante el entrenamiento \cite{FA_1}. 
\par En general el principal requerimiento sobre estas funciones es que sean capaces de respetar el proceso del propagación hacia atrás, no provocando que el gradiente se haga cero lo cual repercutiría negativamente en el proceso del descenso en gradiente. Este es uno de los problemas asociadas a la clasica función sigmoide, dado que para valores de $x$ ampliamente negativos o positivos, provoca que el gradiente sea cero\footnote{Este efecto es conmúnmnte denominado como saturación}, interrumpiendo el descenso en gradiente para la neurona en cuestión y, por tanto, la optimización de sus pesos sinápticos.



\begin{figure}[!hb]
\centering
\includegraphics[scale=0.45]{images/lrelu_2.png}
\caption{(izquierda) Función de activación \textit{Relu}. (Derecha) Función de activación \textit{leakyRelu.} }
\label{fig:CD_2}
\end{figure}

\par Actualmente la función de activación más utilizada es la unidad lineal de rectificación \cite{FA_2} (ReLu del inglés \textit{Rectifier Linear Unit}), representada en la figura \ref{fig:CD_1}. No obstante, otro tipo de función de activación basada en la anteriomente expuesta  denominada unidad lineal de rectificacion con pérdidas (leakyRelu) ha ganado peso en el ámbito. La única diferencia entre ambas funciones es la capacidad de la \textit{leakyRelu} de no hacer nulo el gradiente para valores negativos, ver figura \ref{fig:CD_2} para apreciar esta diferencia. En este proyecto han sido utilizadas tanto la funcion Sigmoide como la función \textit{leakyRelu}


\subsubsection{Topología de Conexionado}
\begin{figure}[!h]
\centering
\includegraphics[scale=0.4]{images/capas.png}
\caption{Esquema de una red neuronal densa de una sola capa oculta }
\label{fig:DNN_4}
\end{figure}


\par Otro concepto determinante en el comportamiento de las redes neuronales es la topología empleada, esto es, el patron de conexionado de una red neuronal. En una red neuronal artificial los nodos se conectan entre sí, siendo este conjunto de conexiones internas junto con los pesos sinápticos lo que determina el comportamiento de la red y, en última instancia, la función asociada  a la red. 
\par Las unidades neuronales suelen agruparse en lo que se denominan capas. La unión de dos o más capas constituyen una red neuronal. Se distinguen tres tipos de capas: de entrada, de salida y ocultas. Una capa de entrada esta compuesta por las neuronas que reciben las señales. Una capa de salida está constituida por el conjunto de neurones que proporcionan la respuesta de la red. Las capas ocultas no tienen conexionado con el exterior. A más capas Socultas más capacidad de aprendizaje tendrá el sistema, aunque el tiempo necesario para su optimización aumentará considerablemente. 


\subsubsection{Propagación Hacia Atrás}

Se denomina propagación hacia atrás al proceso empleado para el entrenamiento de las redes neuronales. Este entrenamiento tiene como objetivo el ajuste de los pesos sinápticos de la red. Se considera un buen ajuste de pesos aquel que minimiza el error a la salida de una red \cite{DNN_training}. De manera breve los principales pasos de este proceso de entrenamiento son:



\begin{figure}[b!]
\centering
\includegraphics[scale=0.30]{images/entrenamiento.png}
\caption{Representación esquemática del proceso de entrenamiento de una red neuronal }
\label{fig:DNN_5}
\end{figure}

\begin{itemize}
\item Inicialización. Se asigna un valor por defecto a los distintos pesos. Se considera un paso determinante, puesto que una mala inicialización puede implicar la saturación de los gradientes en los nodos.
\par Los siguientes pasos constituyen un proceso iterativo, durante el cual se irá minimizando progresivamente el error asociado a la salida de la red. 
\item Paso hacia delante  (\textit{Fast Forward}). Se comprueba el comportamiento de la red, se calcula la salida de la red para un conjunto de muestras de entrada. 
\item Estimación del error de salida. Dada una salida, se evalúa la diferencia con respecto a la salida esperada según las muestras de entrada. 
\item Se realiza la propagación hacia atrás. Dado el error a la salida se realizan el conjunto de derivadas necesarias recorriendo desde la salida hacia la entrada la red, identificando el comportamiento del gradiente del error con respecto a los diferentes pesos de la red.
\item Se modifican los pesos en función del gradiente previamente calculado. 

\end{itemize}



\newpage
\subsection{Red Neuronal Convolucional}

\par Las redes convolucionales (CNN, del inglés \textit{Convolutional Neural Networks}) son una categoría de redes neuronales que se consideran un método altamente eficaz en áreas como el reconocimiento de imágenes \cite{CNN_1}\cite{CNN_2}.
\begin{figure}[]
\centering
\includegraphics[scale=0.50]{images/convNet.png}
\caption{Red convolucional \textit{LeNet5} }
\label{fig:CNN_1}
\end{figure}


\par Este modelo fué introducido en 1989 \cite{CNN} por Yann leCunn, la red de este trabajo fue denominada \textit{LeNet5}. Dicha red se puede observar en la imagen \ref{fig:CNN_1}
\par Las redes convoluciones suelen ser aplicadas a las imágenes. Cada imagen puede ser representada por una matriz de números sí se trata de una imagen en escala de grises, o por tres matrices sí es una imagen a color. Es esta propiedad de las imágenes donde cada dimensión, es decir cada pixel, queda definida espacialmente con respecto al resto de dimensiones, lo que convierte a las imágenes en las muestras ideales para este tipo de red. 
\par Se asume que los conjuntos de pixeles vecions formarán unas características más significativas que sí tomaramos grupos sin tener en cuenta su disposición espacial

a capa anterior lo que se emplea es el operador de convolución.
\begin{figure}[b]
\centering
\includegraphics[scale=0.60]{images/conNet_2.png}
\caption{Ejemplo de aplicación del operador convolución sobre una imagen. Seleccionada una región de la imagen cuyas dimensiones son las mismas que las del kernel seleccionado, se aplica el producto pixel a pixel entre dicha región y los pesos propios del kernel. La suma de estos productos se almacena en la imagen de salida, respetando la ubicación espacial de la región evaluada. }
\label{fig:CNN_2}
\end{figure}

\par Este tipo de redes derivan su nombre del operador de red convolución cuyo objetivo es extraer características de las imágenes preservando la relación espacial entre pixeles.  

\par Dada una imagen bidimensional \textit{I} y una  matriz $K$ de dimensiones $h\times w$ (denominada kernel de convolución) la cual es capaz de extraer algún tipo de característica relevante. La operación de convolución se puede representar como:  


\par Formalmente, se puede expresar como: 
\begin{center}
\begin{equation} \label{eq:CNN_2}
(I*K):{xy} = \sum_{i=1}^{h}\sum_{j=1}^{w} K_{ij}I_{I_{x+i-1},u+j-1}
\end{equation}
\end{center}  
 
\par A diferencia de las redes neuronales convencionales en las redes convolucionales los datos a la entrada y entre el conexionado de capas se agrupan en 3 dimensiones: ancho, alto y profundidad. En este caso nos referimos a "profundidad" por capa no a la profundidad de la red, lo cual se refiere al número de capas  de la red en cuestión. Por ejemplo, dada una imagen de entrada de tres canales (los tres canales de color) de 32x32 pixeles, la agrupación de los datos en la capa de entrada será 32x32x3. Ver figura \ref{fig:CNN_3}
\par Otra diferencia con respecto a la redes neuronales clásicas es que las unidades de una capa solo están conectadas a un espacio reducido de unidades de la capa inmediatamente anterior. 

\begin{figure}[t]
\centering
\includegraphics[scale=0.60]{images/convNet_3.png}
\caption{(Izquierda) Modelo clasico de redes neuronales. (Derecha) Modelo de Red Convolucional. Los datos son reagrupados en 3 dimensionados como se puede observar en una de las capas. Cada una de las capas tiene como entrada una imagen 3D y tiene como salida otra imagen 3D. La capa roja representa la capa de entrada por lo que la altura y la anchura son las dimensiones de la imagen y la profundidad son el número de canales}
\label{fig:CNN_3}
\end{figure}

\par Las redes neuronales convolucionales se fundamentan en tres principios básicos que son los campos receptivos locales, los pesos compartidos y el empleo de agrupaciones o \textit{pooling}

\subsubsection{Filtros Locales}

\par En una red neuronal densa, esto es, una red totalmente conectada como la de imagen \ref{fig:CNN_3} las entradas se interpretan como un conjunto "vertical" de unidades. Sin embargo, en un red convolucional es preferible organizar las unidades de entrada en forma bidimensional.
\begin{figure}[t]
\centering
\includegraphics[scale=0.60]{images/conNet_4.png}
\caption{Representación de la conectividad local en una red neuronal}
\label{fig:CNN_4}
\end{figure}

\par Las capas consecutivas estarán conectadas entre sí, pero cada unidad de una capa oculta estará conectada solo a un conjunto de unidades de la capa inmediatamente anterior.
\begin{figure}[b]
\centering
\includegraphics[scale=0.60]{images/ConvNet_6.png}
\caption{Desplazamiento del campo de recepción}
\label{fig:CNN_5}
\end{figure}


\par Se denomina filtro local a la ventana que se aplica a las diferentes regiones seleccionables de la imagen, cada una de estas régiones seleccionables estan conectadas a una única unidad de la siguiente capa oculta. A este término a menudo nos referimos como kernel. Este filtro se desplazará por toda la imagen, realizando el proceso de convolución por toda ella, ver imagen \ref{fig:CNN_5}. Es esto lo que permite extraer características de manera local por toda la imagen
\par Normalmente el desplazamiento se hace pixel a pixel aunque es posible aumentar el número de pixeles por desplazamiento. Este hiperparámetro se denomina generalmente \textit{stride}. En este trabajo se ha utilizado un valor de dos. Otro concepto a tener en cuenta es que por lo general hay varios tipos de fitros para la extracción de características en las diferentes capas.  

\subsubsection{Pesos Compartidos}

\par Cada uno de los filtros de recepción serán aplicados a toda la imagen con el mismo peso para todas las diferentes regiones. Esto significa que el patrón de selección de características será el mismo, por lo que las neuronas de la siguiente capa detectarán el mismo tipo de característica. 
\par El punto anterior se fundamente en que generalmente un patrón  de una parte de la imagen es probable que se repita en otra parte de la imagen dada la propia naturaleza de las imágenes. 
\par Con objeto de no limitar cada capa a la extracción de un tipo de característicia se aplican numeros filtros en cada una de las capas de convolución. Gracias a esto se consiguen extraer distintos tipos de patrones. 

\begin{figure}[t]
\centering
\includegraphics[scale=0.40]{images/conv_net7.png}
\caption{Representación de la extracción de varias características con varios filtros}
\label{fig:CNN_6}
\end{figure}
\par Una de la ventajas del uso de pesos compartidos es que permite reducir el número de parámetros de la red. 


\begin{figure}[t]
\centering
\includegraphics[scale=0.50]{images/convNet_7.png}
\caption{Representación del proceso de agrupamiento (\textit{pooling})}
\label{fig:CNN_}
\end{figure}

\subsubsection{Agrupamiento}

\par Otro tipo de capa característica de las redes convolucionales son las capas de agrupamiento o \textit{pooling}.  Esta capa tiene como objetivo reducir el número de datos generado, realizando una estimación del valor más importante de una determinada region. Esto permite reducir progresivamente el tamaño de la imagen. Este proceso de agrupamiento se aplica individualmente a cada una de las imagnenes generadas por cada filtro.  
\par No obstante, esta funcionalidad no ha sido utilizada en el modelo generado en este trabajo dado que actualmente la librería empleada (\textit{TensorFlow})  no tiene implementada esta operación para imágenes 3D. 




\newpage
\section{Herramientas Complementarias}
\subsection{Máquina de Vectores de Soporte}
\subsection{Validación Cruzada (K-fold)}
\subsection{Métricas de Validación}







































\chapterend{}